{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a13e6bf3-a6ce-4cbd-982b-bc002b50cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fef8312-5a96-4df6-b88c-b083a385bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/jchang153/Documents/UCLA-CAM/GV KG/Data/Tweets/All/bigbabies/bigbaby_r_t_30_s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1bb5621-0088-4dc0-abf1-6e30910b9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = pd.read_csv('user_network_black.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bdcbf3-8d9a-4ebf-8f4c-98a8e3ac00e2",
   "metadata": {},
   "source": [
    "## Community Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ee738-976c-4e0b-84ff-6f8c7b16d274",
   "metadata": {},
   "source": [
    "Trying to find centrality for each of the communities created by Charlie. But I don't have monthly centralities, I am just using df_c for now. I need to run monthly centrality later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8025ca-61e5-45ec-b89c-b8bd30aeeb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_month = pd.Timestamp('2020-03')\n",
    "end_month = pd.Timestamp('2021-06')\n",
    "all_months = pd.period_range(start_month, end_month, freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d6b5b-3c1e-443e-bc31-4a8333b50496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03\n",
      "2020-04\n",
      "2020-05\n",
      "2020-06\n",
      "2020-07\n",
      "2020-08\n",
      "2020-09\n",
      "2020-10\n",
      "2020-11\n",
      "2020-12\n",
      "2021-01\n",
      "2021-02\n",
      "2021-03\n",
      "2021-04\n",
      "2021-05\n",
      "2021-06\n"
     ]
    }
   ],
   "source": [
    "for month in all_months:\n",
    "    print(str(month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "e9615c53-987b-4bb7-be96-96c166ca68b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                            | 0/16 [00:00<?, ?it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 57.28it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 39.82it/s]\u001b[A\n",
      " 12%|██████████████▌                                                                                                     | 2/16 [00:00<00:00, 18.01it/s]\n",
      "  0%|                                                                                                                            | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 31%|███████████████████████████████████▋                                                                                | 4/13 [00:00<00:00, 32.80it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████████████████████████████▍                                            | 8/13 [00:00<00:00, 33.81it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 28.03it/s]\u001b[A\n",
      "\n",
      "  0%|                                                                                                                            | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|████████████████▌                                                                                                   | 3/21 [00:00<00:00, 18.47it/s]\u001b[A\n",
      " 24%|███████████████████████████▌                                                                                        | 5/21 [00:00<00:00, 16.20it/s]\u001b[A\n",
      " 33%|██████████████████████████████████████▋                                                                             | 7/21 [00:00<00:00, 14.69it/s]\u001b[A\n",
      " 43%|█████████████████████████████████████████████████▋                                                                  | 9/21 [00:00<00:00, 14.04it/s]\u001b[A\n",
      " 52%|████████████████████████████████████████████████████████████▏                                                      | 11/21 [00:00<00:00, 14.97it/s]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████████████████████████▋                                      | 14/21 [00:00<00:00, 18.06it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:01<00:00, 20.46it/s]\u001b[A\n",
      " 25%|█████████████████████████████                                                                                       | 4/16 [00:01<00:05,  2.10it/s]\n",
      "  0%|                                                                                                                             | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 34.99it/s]\u001b[A\n",
      " 31%|████████████████████████████████████▎                                                                               | 5/16 [00:01<00:04,  2.46it/s]\n",
      "  0%|                                                                                                                             | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 30.98it/s]\u001b[A\n",
      " 38%|███████████████████████████████████████████▌                                                                        | 6/16 [00:02<00:03,  2.89it/s]\n",
      "  0%|                                                                                                                             | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 37.01it/s]\u001b[A\n",
      " 44%|██████████████████████████████████████████████████▊                                                                 | 7/16 [00:02<00:02,  3.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 50.99it/s]\u001b[A\n",
      " 50%|██████████████████████████████████████████████████████████                                                          | 8/16 [00:02<00:01,  4.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 47.60it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 52.04it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████████████████████████████▉                                           | 10/16 [00:02<00:00,  6.27it/s]\n",
      "  0%|                                                                                                                             | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 34.69it/s]\u001b[A\n",
      " 69%|███████████████████████████████████████████████████████████████████████████████                                    | 11/16 [00:02<00:00,  6.34it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 51.93it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 59.56it/s]\u001b[A\n",
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████▍                     | 13/16 [00:02<00:00,  8.77it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 36.74it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:02<00:00,  5.58it/s]\n"
     ]
    }
   ],
   "source": [
    "path = '../charlie_communities_black/Users Community in triples_'\n",
    "\n",
    "for month in tqdm(all_months):\n",
    "    df_com = pd.read_csv(path + str(month).replace('-', '_') + '.csv')\n",
    "    df_com['user_id'] = df_com['user_id'].apply(ast.literal_eval)\n",
    "    df_com = df_com[df_com['user_id'].apply(lambda x: len(x) >= 100)]\n",
    "    com_centrs = []\n",
    "    for com in tqdm(df_com['user_id']):\n",
    "        temp = {}\n",
    "        for user in com:\n",
    "            if user in list(df_c['user']):\n",
    "                # print('here')\n",
    "                temp[user] = df_c[df_c['user'] == user].iloc[0]['eigenvector centrality']\n",
    "        com_centrs.append(temp)\n",
    "    df_com['centralities'] = com_centrs\n",
    "    df_com.to_csv(path + str(month).replace('-', '_') + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a287ac5-2039-4c49-b5e1-f4acb07394b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f5b170-e4c1-411d-b8f7-9bdc49104aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a6f649-f805-4dfc-9ee6-9551b1288901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6933566-625e-493e-a24c-8c30318bc379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e91debd-cd9b-4fc7-b5c6-203554131c1b",
   "metadata": {},
   "source": [
    "## Running Louvain on my user network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c571e2-1e61-49f0-963a-6e5fbafe7023",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.Timestamp('2020-03-11')\n",
    "end_date = pd.Timestamp('2021-06-16')\n",
    "# omit 2021-06-17 because it's a thursday, and we want nice weeks bc 2020-03-11 starts on wednesday\n",
    "\n",
    "biweekly_dates = pd.date_range(start=start_date, end=end_date, freq='2W-WED')\n",
    "biweekly_dates = biweekly_dates.union([end_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "03b5b81c-a181-409b-8be1-06115e8a6794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "a56a0a66-532b-47fc-adc1-8ccc5299cb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [00:33<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "for date in tqdm(biweekly_dates[:-1]):\n",
    "    with open(f'./usersby2w-black/relations_{str(date)[:10]}.pkl', 'rb') as f:\n",
    "        triples = pickle.load(f)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for u, v, w in triples:\n",
    "        G.add_edge(u, v, weight=w)\n",
    "\n",
    "    partition = community.best_partition(G, weight='weight')\n",
    "    community_data = {'community_id': [], 'users': [], 'user_count': []}\n",
    "\n",
    "    for node, comm_id in partition.items():\n",
    "        if comm_id not in community_data['community_id']:\n",
    "            community_data['community_id'].append(comm_id)\n",
    "            community_data['users'].append([node])\n",
    "            community_data['user_count'].append(1)\n",
    "        else:\n",
    "            idx = community_data['community_id'].index(comm_id)\n",
    "            community_data['users'][idx].append(node)\n",
    "            community_data['user_count'][idx] += 1\n",
    "\n",
    "    df_communities = pd.DataFrame(community_data)\n",
    "    df_communities['avg eigenvector centrality'] = [np.mean(df_c[df_c['user'].isin(user_list)]['eigenvector centrality']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg weighted degree'] = [np.mean(df_c[df_c['user'].isin(user_list)]['weighted degree']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg followers'] = [np.mean(df_c[df_c['user'].isin(user_list)]['followers']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg sent'] = [np.mean(df_c[df_c['user'].isin(user_list)]['average sent']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg num tweets'] = [np.mean(df_c[df_c['user'].isin(user_list)]['num tweets']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg num strong tweets'] = [np.mean(df_c[df_c['user'].isin(user_list)]['num strong tweets']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg num negative tweets'] = [np.mean(df_c[df_c['user'].isin(user_list)]['num negative tweets']) for user_list in list(df_communities['users'])]\n",
    "\n",
    "    df_communities.to_csv('./usersby2w-black/user_communities_' + str(date)[:10] + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "9f13d8b2-952e-41d8-a068-b840e6fa3e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_asdf = pd.read_csv('./usersby2w-black/user_communities_2020-12-30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "88c56a58-3df1-4880-a2bc-ba1cd8d72956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>community_id</th>\n",
       "      <th>users</th>\n",
       "      <th>user_count</th>\n",
       "      <th>avg eigenvector centrality</th>\n",
       "      <th>avg weighted degree</th>\n",
       "      <th>avg followers</th>\n",
       "      <th>avg sent</th>\n",
       "      <th>avg num tweets</th>\n",
       "      <th>avg num strong tweets</th>\n",
       "      <th>avg num negative tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[8.620798671012946e+17, 1.0226466006596936e+18...</td>\n",
       "      <td>193</td>\n",
       "      <td>0.021730</td>\n",
       "      <td>566.209650</td>\n",
       "      <td>3929.994819</td>\n",
       "      <td>-0.068242</td>\n",
       "      <td>27.823834</td>\n",
       "      <td>9.253886</td>\n",
       "      <td>6.191710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[1.3158682071739556e+18, 28874068.0, 15723895....</td>\n",
       "      <td>35</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>532.599425</td>\n",
       "      <td>2495.457143</td>\n",
       "      <td>-0.096786</td>\n",
       "      <td>22.685714</td>\n",
       "      <td>8.485714</td>\n",
       "      <td>5.742857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[1.0835734352660972e+18, 48899468.0, 7.4235446...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.023010</td>\n",
       "      <td>589.383352</td>\n",
       "      <td>13288.076923</td>\n",
       "      <td>-0.050492</td>\n",
       "      <td>24.769231</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[199120695.0, 3385139536.0, 17739546.0, 231313...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.062996</td>\n",
       "      <td>1642.187116</td>\n",
       "      <td>2463.125000</td>\n",
       "      <td>-0.099596</td>\n",
       "      <td>73.750000</td>\n",
       "      <td>37.750000</td>\n",
       "      <td>27.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[473384312.0, 9.182730501596242e+17, 90344035....</td>\n",
       "      <td>9</td>\n",
       "      <td>0.035629</td>\n",
       "      <td>895.786097</td>\n",
       "      <td>854.555556</td>\n",
       "      <td>-0.102428</td>\n",
       "      <td>34.555556</td>\n",
       "      <td>9.666667</td>\n",
       "      <td>6.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[420025937.0, 4786423813.0, 39100425.0, 9.7116...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.039958</td>\n",
       "      <td>1011.343465</td>\n",
       "      <td>1982.000000</td>\n",
       "      <td>-0.012150</td>\n",
       "      <td>35.210526</td>\n",
       "      <td>12.842105</td>\n",
       "      <td>8.526316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   community_id                                              users  \\\n",
       "0             0  [8.620798671012946e+17, 1.0226466006596936e+18...   \n",
       "1             1  [1.3158682071739556e+18, 28874068.0, 15723895....   \n",
       "2             2  [1.0835734352660972e+18, 48899468.0, 7.4235446...   \n",
       "3             3  [199120695.0, 3385139536.0, 17739546.0, 231313...   \n",
       "4             4  [473384312.0, 9.182730501596242e+17, 90344035....   \n",
       "5             5  [420025937.0, 4786423813.0, 39100425.0, 9.7116...   \n",
       "\n",
       "   user_count  avg eigenvector centrality  avg weighted degree  avg followers  \\\n",
       "0         193                    0.021730           566.209650    3929.994819   \n",
       "1          35                    0.021018           532.599425    2495.457143   \n",
       "2          13                    0.023010           589.383352   13288.076923   \n",
       "3           8                    0.062996          1642.187116    2463.125000   \n",
       "4           9                    0.035629           895.786097     854.555556   \n",
       "5          19                    0.039958          1011.343465    1982.000000   \n",
       "\n",
       "   avg sent  avg num tweets  avg num strong tweets  avg num negative tweets  \n",
       "0 -0.068242       27.823834               9.253886                 6.191710  \n",
       "1 -0.096786       22.685714               8.485714                 5.742857  \n",
       "2 -0.050492       24.769231               8.000000                 5.076923  \n",
       "3 -0.099596       73.750000              37.750000                27.250000  \n",
       "4 -0.102428       34.555556               9.666667                 6.555556  \n",
       "5 -0.012150       35.210526              12.842105                 8.526316  "
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "bfa22c8d-e685-43c0-8d0d-b5381b45ce65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[199120695,\n",
       " 3385139536,\n",
       " 17739546,\n",
       " 2313136440,\n",
       " 1337092240813006848,\n",
       " 109360451,\n",
       " 41747307,\n",
       " 1325838919045296128]"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int(x) for x in ast.literal_eval(df_asdf.iloc[3]['users'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "cb49ffc9-50c2-4fa7-8fcc-101139f150aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“the police song” is arranged, produced, and written by dan devon. and inspired by the blacklivesmatter movement.… https://t.co/o3n4zjh5wq\n",
      "riots won’t bring george floyd back or make the communities any safer or better. we’re living in the inevitable. there will be more blood.\n",
      "buying into the media’s narrative that the georgefloyd tragedy is a black vs white thing is just as bad for you as… https://t.co/t4fcm7gyil\n",
      "@lanayalewis now this was a racially motivated crime. i’m still not convinced about breanna taylor and george floyd tragedies.\n",
      "@lanayalewis now this was a racially motivated crime. i’m still not convinced that breonna taylor and george floyd… https://t.co/xcyipabr4h\n",
      "i get it but at the same time i don’t. why didn't the media or blacklivesmatter  highlight this tragedy?\n",
      "i get it but at the same time i don’t. why didn't the media or blacklivesmatter   highlight this tragedy?… https://t.co/68pj5xlsp4\n",
      "i hope joe biden doesn’t think he’s gonna win the black vote by promising us reparations in the form of popeyes and ben &amp; jerry gift cards.\n",
      "“the problem with the george floyd narrative” via ⁦@medium⁩\n",
      "https://t.co/8up1sd5vef\n",
      "it’s impossible to get off the slave ship if you truly believe the lie that black people cant swim\n",
      "it’s impossible to get off the slave ship if you truly believe the lie that black people can’t swim\n",
      "“the problem with the george floyd narrative” \n",
      "georgefloyd policebrutality\n",
      "https://t.co/dhxnmgeypb\n",
      "“the problem with the george floyd narrative” georgefloyd justiceforgeorgefloyd policebrutality\n",
      "https://t.co/bejfztuxda\n",
      "“the problem with the georgefloyd narrative” via ⁦@medium⁩ opinion writer\n",
      "https://t.co/kpypzp44ka\n",
      "“the problem with the georgefloyd narrative” justiceforfloyd \n",
      "https://t.co/8uxaapgozh\n",
      "“the problem with the georgefloyd narrative” \n",
      "writer blog blogger \n",
      "https://t.co/thtczj6hbb\n",
      "“the problem with the georgefloyd narrative” \n",
      "https://t.co/nhltjai7tq\n",
      "the next slave film better include lightsabers or we’re not making any damn progress at all\n",
      "muhammad ali's son says dad would have hated 'racist' black lives matter https://t.co/uxyg0omuuw\n",
      "and yet another slave film...\n",
      "here’s an article i wrote about how a day without police would look like for la defundthepolice blacklivesmatter \n",
      "https://t.co/rxfifyrjct\n",
      "how would la look with no police? defundthepolice blacklivesmatter\n",
      "i could have sworn we’d get through this thing without financing another slave film\n",
      "these numbers are ridiculous. the illuminati needs to tell america the truth and admit that covid wears black air f… https://t.co/egmwpscgjv\n",
      "black unity occupiers attempting to bring chaz to grand park writer blacklivesmatter \n",
      "https://t.co/pyrt1cla00\n",
      "news black unity 24/7 occupiers attempt to bring chop to los angeles blacklivesmatter writers  https://t.co/pyrt1cla00\n",
      "black unity 24/7 occupiers seek to bring chop to los angeles blacklivesmatter defundthepolice  https://t.co/pyrt1cla00\n",
      "niggas stay on the slave ship bc they’re dumb enough to actually believe that niggas can’t swim\n",
      "i’m all for reparations as long as they don’t come in the form of ben &amp; jerry’s and popeye’s chicken rebates\n",
      "imagine telling a slave that a black person went to space...\n",
      "imagine telling a slave that a black man was president of the united states\n",
      "wow, i never thought i’d see the day when uncle ben would finally ditch his slave name.\n",
      "wow, i never thought i’d see the day when uncle ben would finally ditch his slave name. https://t.co/a3xiki0rjv\n",
      "the problem with the george floyd narrative  https://t.co/vueyxqctzn\n",
      "imagine telling a slave that a black person went to space\n",
      "these $2000 checks have become the new reparations\n",
      "derek chauvin is a pos but nobody can prove what he did was racially motivated. yet somehow he’s created a new job… https://t.co/llid2nsoj9\n"
     ]
    }
   ],
   "source": [
    "for i in df[df['user_id'] == 3385139536]['text']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8d7b5-4aba-495c-860e-96ecdd2980d6",
   "metadata": {},
   "source": [
    "## Running Louvain on the thresholded user network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ca8dee-4c01-4d70-bc8e-c32994144b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.Timestamp('2020-03-11')\n",
    "end_date = pd.Timestamp('2021-06-16')\n",
    "# omit 2021-06-17 because it's a thursday, and we want nice weeks bc 2020-03-11 starts on wednesday\n",
    "\n",
    "biweekly_dates = pd.date_range(start=start_date, end=end_date, freq='2W-WED')\n",
    "biweekly_dates = biweekly_dates.union([end_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd7579fa-56f4-47ec-82a5-2548804456d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd8901a-4dc7-46ae-ae7a-7417c6e43264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                            | 0/33 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './usersby2w-black-threshold/relations_2020-03-11.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m tqdm(biweekly_dates[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./usersby2w-black-threshold/relations_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m         triples \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      5\u001b[0m     G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './usersby2w-black-threshold/relations_2020-03-11.pkl'"
     ]
    }
   ],
   "source": [
    "for date in tqdm(biweekly_dates[:-1]):\n",
    "    with open(f'./usersby2w-black-threshold/relations_{str(date)[:10]}.pkl', 'rb') as f:\n",
    "        triples = pickle.load(f)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for u, v, w in triples:\n",
    "        if w >= 0.1:\n",
    "            G.add_edge(u, v, weight=w)\n",
    "\n",
    "    partition = community.best_partition(G, weight='weight')\n",
    "    community_data = {'community_id': [], 'users': [], 'user_count': []}\n",
    "\n",
    "    for node, comm_id in partition.items():\n",
    "        if comm_id not in community_data['community_id']:\n",
    "            community_data['community_id'].append(comm_id)\n",
    "            community_data['users'].append([node])\n",
    "            community_data['user_count'].append(1)\n",
    "        else:\n",
    "            idx = community_data['community_id'].index(comm_id)\n",
    "            community_data['users'][idx].append(node)\n",
    "            community_data['user_count'][idx] += 1\n",
    "\n",
    "    df_communities = pd.DataFrame(community_data)\n",
    "    df_communities['avg eigenvector centrality'] = [np.mean(df_c[df_c['user'].isin(user_list)]['eigenvector centrality']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg weighted degree'] = [np.mean(df_c[df_c['user'].isin(user_list)]['weighted degree']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg followers'] = [np.mean(df_c[df_c['user'].isin(user_list)]['followers']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg sent'] = [np.mean(df_c[df_c['user'].isin(user_list)]['average sent']) for user_list in list(df_communities['users'])]\n",
    "    # df_communities['avg num tweets'] = [np.mean(df_c[df_c['user'].isin(user_list)]['num tweets']) for user_list in list(df_communities['users'])]\n",
    "    # df_communities['avg num strong tweets'] = [np.mean(df_c[df_c['user'].isin(user_list)]['num strong tweets']) for user_list in list(df_communities['users'])]\n",
    "    # df_communities['avg num negative tweets'] = [np.mean(df_c[df_c['user'].isin(user_list)]['num negative tweets']) for user_list in list(df_communities['users'])]\n",
    "\n",
    "    df_communities.to_csv('./usersby2w-black-threshold/user_communities_' + str(date)[:10] + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823b8f7-5471-4ac2-81ae-4897a7bcd86b",
   "metadata": {},
   "source": [
    "## Running Louvain on my user network all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd2a585-422e-43aa-982d-e6594f3cb9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/jchang153/Documents/UCLA-CAM/GV KG/Data/Tweets/All/bigbabies/bigbaby_r_t_30_s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953735a7-f33e-449e-babb-04720431beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = pd.read_csv('./user networks/user_network_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff5e062-4a1b-4f8f-91f2-b24398550935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user', 'eigenvector centrality', 'weighted degree', 'num tweets',\n",
       "       'followers', 'screen name', 'average sent', 'num strong tweets',\n",
       "       'num negative tweets', 'prop strong tweets', 'prop negative tweets',\n",
       "       'prod', 'logprod'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49d15dd3-5f24-4c66-a8ba-1abb114744fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.Timestamp('2020-03-11')\n",
    "end_date = pd.Timestamp('2021-06-16')\n",
    "# omit 2021-06-17 because it's a thursday, and we want nice weeks bc 2020-03-11 starts on wednesday\n",
    "\n",
    "biweekly_dates = pd.date_range(start=start_date, end=end_date, freq='2W-WED')\n",
    "biweekly_dates = biweekly_dates.union([end_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a405e9bd-f9df-4239-9fc9-546989dca8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b6e43-e557-4b6d-9370-bfb2587edc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d50081-9805-4782-8709-0a0ca9d63f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [00:36<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "for date in tqdm(biweekly_dates[:-1]):\n",
    "    with open(f'./usersby2w-all/relations_{str(date)[:10]}.pkl', 'rb') as f:\n",
    "        triples = pickle.load(f)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for u, v, w in triples:\n",
    "        G.add_edge(u, v, weight=w)\n",
    "\n",
    "    partition = community.best_partition(G, weight='weight')\n",
    "    community_data = {'community_id': [], 'users': [], 'user_count': []}\n",
    "\n",
    "    for node, comm_id in partition.items():\n",
    "        if comm_id not in community_data['community_id']:\n",
    "            community_data['community_id'].append(comm_id)\n",
    "            community_data['users'].append([node])\n",
    "            community_data['user_count'].append(1)\n",
    "        else:\n",
    "            idx = community_data['community_id'].index(comm_id)\n",
    "            community_data['users'][idx].append(node)\n",
    "            community_data['user_count'][idx] += 1\n",
    "\n",
    "    df_communities = pd.DataFrame(community_data)\n",
    "    df_communities['avg eigenvector centrality'] = [np.mean(df_c[df_c['user'].isin(user_list)]['eigenvector centrality']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg weighted degree'] = [np.mean(df_c[df_c['user'].isin(user_list)]['weighted degree']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg followers'] = [np.mean(df_c[df_c['user'].isin(user_list)]['followers']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg sent'] = [np.mean(df_c[df_c['user'].isin(user_list)]['average sent']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg num tweets'] = [np.mean(df_c[df_c['user'].isin(user_list)]['num tweets']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg num strong tweets'] = [np.mean(df_c[df_c['user'].isin(user_list)]['num strong tweets']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg num negative tweets'] = [np.mean(df_c[df_c['user'].isin(user_list)]['num negative tweets']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg prop strong tweets'] = [np.mean(df_c[df_c['user'].isin(user_list)]['prop strong tweets']) for user_list in list(df_communities['users'])]\n",
    "    df_communities['avg prop negative tweets'] = [np.mean(df_c[df_c['user'].isin(user_list)]['prop negative tweets']) for user_list in list(df_communities['users'])]\n",
    "\n",
    "    df_communities.to_csv('./usersby2w-all/user_communities_' + str(date)[:10] + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
